# ADR 002: Data Version Control

## Status
Proposed

## Context
Large files produced by data processing or model training cannot be efficiently stored in Git.
Git lacks deduplication and content addressing for binary data, leading to unmanageable repository sizes.
However, the system requires a reproducible, auditable, and Git-integrated method for managing artifacts generated by orchestrated jobs.

## Decision
Adopt **DVC (Data Version Control)** for artifact versioning and storage.
DVC extends Git by storing lightweight metadata in the repository while maintaining large files in external object storage.

### Key Properties
- `.dvc` files in Git record the version and location of large artifacts.
- Actual data is stored in DVC remotes (e.g., MinIO or S3).
- Multiple DVC remotes can be configured to represent different storage tiers (internal vs. publication).
- Artifacts are tracked and deduplicated by content hash, ensuring consistency and integrity.

### Implementation Pattern
1. **Node-side Materialization**
  - Dagster materializes assets (e.g., `outputs/model.pkl`) to internal MinIO storage.
  - Node tracks outputs using DVC:
    ```bash
    dvc add outputs/model.pkl
    dvc push -r internal-storage
    ```

2. **Governed Publication**
  - Node commits `.dvc` metafiles to the Ctrl Repo and raises a PR.
  - Upon approval and merge, Node promotes data to publication storage:
    ```bash
    dvc pull -r internal-storage
    dvc push -r publication-storage
    ```

3. **Access by Researchers**
  - Researchers retrieve approved results using standard Git+DVC commands:
    ```bash
    git pull
    dvc pull-r publication-storage
    ```

## Consequences
- Provides reproducible, verifiable artifact tracking linked to Git commits.
- Supports approval-based promotion of results from internal to public storage.
- Keeps Git repositories lightweight and efficient.
- Adds operational overhead for maintaining DVC remotes and cache storage.


## Alternatives Considered
Alternatives do not natively support multiple, independently addressable remote storage locations that can be mapped per project or artifact,
at least not with the same flexibility or automation as DVC.

- **direct git storage of artifacts** - rejected for scalability and performance reasons.
- **pure mlflow artifact management** - rejected due to lack of Git integration for approval workflows (could be used in future alongside DVC)
- **git-annex / datalad** - challenging to manage multiple remote data remotes and provides limited governance or support for programmatic use.
- **git-lfs** - Only one LFS endpoint is active per repository; per-project remotes are not supported.

git-annex / Datalad - rejected because they are optimized for distributed file synchronization rather than automated data promotion within CI/CD pipelines.
Both tools rely on manual command flows or out-of-band sync mechanisms that are difficult to coordinate with GitHub Actions or Argo-based workflows.
They lack simple primitives for remote-to-remote transfers (e.g., pulling from internal MinIO and pushing to publication S3), which DVC supports natively with dvc pull / dvc push.

Additionally, DVC's content-addressed cache structure and Python API allow easier integration with orchestrators such as Dagster or MLflow for artifact lineage tracking,
whereas git-annex and Datalad treat file provenance as secondary metadata and require extra plumbing to expose it programmatically.

## Case Study

###  HuggingFace

### Why Hugging Face Used Git LFS
Hugging Face initially relied on **Git LFS** to version large model and dataset files.
Git LFS stores large binaries outside the Git repo, replacing them with lightweight pointers.
This allowed early Hub users to manage large assets with standard Git workflows and full provenance tracking.
However, Git LFS deduplicates only at the file level, small changes require re-uploading the entire file-making it inefficient at massive scale.

### Why They Switched to Xet
As the Hub grew beyond **20 PB** of data across hundreds of thousands of repos, Git LFS's file-level model became a bottleneck. \
Hugging Face adopted **Xet**, a content-addressed system using chunk-level deduplication and delta transfers. \
Xet improved transfer speeds, reduced storage costs, and supported global deduplication across all repositories-an essential optimization for a public, multi-petabyte platform.

### Why We Should Adopt DVC Instead
Our architecture operates at a smaller, federated scale focused on governance, auditability, and reproducibility-not mass public hosting. **DVC** provides the benefits of data versioning without the operational overhead of custom storage layers (Xet):

- **Versioning code and data:** Tracks datasets and models alongside code using lightweight Git metadata without bloating repositories.
- **Governance alignment:** Integrates with PR-based workflows, ArgoCD, and audit pipelines for controlled deployment.
- **Right scale:** Suited for our moderate data volumes without the overhead of large-scale systems like Xet.
- **Low complexity:** Avoids custom tooling and maintenance burden while fitting into existing infrastructure.
- **Reproducibility:** Links code, data, and models in a single version history for compliance and auditability.
